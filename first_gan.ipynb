{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f137abb24f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
    "    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    return image_grid\n",
    "    # plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "class Generator(nn.Module):\n",
    "    # z_dim : dimension of noise, scalar\n",
    "    # im_dim: dimension of images: image(28x28 = 784)\n",
    "    # hidden_dim: output of hidden layer, s scalar\n",
    "    def __init__(self, z_dim = 10, im_dim = 784, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            get_generator_block(z_dim, hidden_dim),\n",
    "            get_generator_block(hidden_dim, hidden_dim*2),\n",
    "            get_generator_block(hidden_dim*2, hidden_dim*4),\n",
    "            get_generator_block(hidden_dim*4, hidden_dim*8),\n",
    "            nn.Linear(hidden_dim*8, im_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "    \n",
    "    def get_model_gennerator(self):\n",
    "        return self.gen\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(n, z_dim, device='cpu'):\n",
    "    # Create noise\n",
    "    # n - the number of samples to generate\n",
    "    # z_dim - the dimension of noise vector, scalar\n",
    "    return torch.randn(n, z_dim, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dicriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dis_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.LeakyReLU(0.2)\n",
    "    )\n",
    "class Discriminator(nn.Module):\n",
    "    # dimension of images: 28x28 = 784\n",
    "    def __init__(self, im_dim = 784, hidden_dim = 128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            get_dis_block(im_dim, hidden_dim*4),\n",
    "            get_dis_block(hidden_dim*4, hidden_dim*2),\n",
    "            get_dis_block(hidden_dim*2,hidden_dim),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "    def forward(self, image):\n",
    "        return self.model(image)\n",
    "    \n",
    "    def get_model_discriminator(self):\n",
    "        return self.model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# epochs \n",
    "epochs = 20\n",
    "z_dim = 64 # dimension of the noise vector\n",
    "display_step = 500\n",
    "batch_size = 128\n",
    "lr = 0.00002\n",
    "# Em dung MNIST dataset\n",
    "dataloader = DataLoader(\n",
    "    MNIST('.', download=False, transform=transforms.ToTensor()),\n",
    "    batch_size= batch_size,\n",
    "    shuffle= True\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer_fake = SummaryWriter(f\"runs/GAN_MNIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/GAN_MNIST/real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(z_dim=z_dim).to(device)\n",
    "gen_optimazation = torch.optim.Adam(generator.parameters(), lr = lr)\n",
    "discriminator = Discriminator().to(device=device)\n",
    "disc_optimazation = torch.optim.Adam(discriminator.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to caculate discriminator's loss and generator's loss\n",
    "def get_gen_loss(generator, discriminator, criterion, num_images, z_dim, device):\n",
    "    # Create noise\n",
    "    fake_noise = get_noise(num_images, z_dim, device)\n",
    "    fake = generator(fake_noise)\n",
    "    disc_fake_pred = discriminator(fake)\n",
    "    gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "    return gen_loss\n",
    "\n",
    "def get_dis_loss(generator, discriminator, criterion, real, num_images, z_dim, device):\n",
    "    # fake\n",
    "    fake_noise = get_noise(num_images, z_dim=z_dim, device=device)\n",
    "    fake = generator(fake_noise)\n",
    "    disc_fake_predict = discriminator(fake.detach())\n",
    "    disc_fake_loss = criterion(disc_fake_predict, torch.zeros_like(disc_fake_predict))\n",
    "    # real\n",
    "    dis_real_pred = discriminator(real)\n",
    "    disc_real_loss = criterion(dis_real_pred, torch.ones_like(dis_real_pred))\n",
    "    # loss\n",
    "    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20] Batch 0/469                       Loss D: 0.6885, loss G: 0.7349\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "size of input tensor and input format are different.         tensor shape: (128, 784), input_format: CHW",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m mean_generator_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     56\u001b[0m mean_discriminator_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 58\u001b[0m writer_fake\u001b[39m.\u001b[39;49madd_image(\n\u001b[1;32m     59\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mMnist Fake Images\u001b[39;49m\u001b[39m\"\u001b[39;49m, fake, global_step\u001b[39m=\u001b[39;49mcur_step\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m writer_real\u001b[39m.\u001b[39madd_image(\n\u001b[1;32m     62\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mMnist Real Images\u001b[39m\u001b[39m\"\u001b[39m, img_grid_real, global_step\u001b[39m=\u001b[39mcur_step\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m cur_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/GAN_REARCH_2023/env/lib/python3.10/site-packages/torch/utils/tensorboard/writer.py:615\u001b[0m, in \u001b[0;36mSummaryWriter.add_image\u001b[0;34m(self, tag, img_tensor, global_step, walltime, dataformats)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcaffe2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m workspace\n\u001b[1;32m    613\u001b[0m     img_tensor \u001b[39m=\u001b[39m workspace\u001b[39m.\u001b[39mFetchBlob(img_tensor)\n\u001b[1;32m    614\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_summary(\n\u001b[0;32m--> 615\u001b[0m     image(tag, img_tensor, dataformats\u001b[39m=\u001b[39;49mdataformats), global_step, walltime\n\u001b[1;32m    616\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/GAN_REARCH_2023/env/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py:440\u001b[0m, in \u001b[0;36mimage\u001b[0;34m(tag, tensor, rescale, dataformats)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Outputs a `Summary` protocol buffer with images.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39mThe summary has up to `max_images` summary values containing images. The\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39mimages are built from `tensor` which must be 3-D with shape `[height, width,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39m  buffer.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    439\u001b[0m tensor \u001b[39m=\u001b[39m make_np(tensor)\n\u001b[0;32m--> 440\u001b[0m tensor \u001b[39m=\u001b[39m convert_to_HWC(tensor, dataformats)\n\u001b[1;32m    441\u001b[0m \u001b[39m# Do not assume that user passes in values in [0, 255], use data type to detect\u001b[39;00m\n\u001b[1;32m    442\u001b[0m scale_factor \u001b[39m=\u001b[39m _calc_scale_factor(tensor)\n",
      "File \u001b[0;32m~/Desktop/GAN_REARCH_2023/env/lib/python3.10/site-packages/torch/utils/tensorboard/_utils.py:102\u001b[0m, in \u001b[0;36mconvert_to_HWC\u001b[0;34m(tensor, input_format)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_HWC\u001b[39m(tensor, input_format):  \u001b[39m# tensor: numpy array\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(input_format)) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m     97\u001b[0m         input_format\n\u001b[1;32m     98\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mYou can not use the same dimension shordhand twice. \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m        input_format: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    100\u001b[0m         input_format\n\u001b[1;32m    101\u001b[0m     )\n\u001b[0;32m--> 102\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(tensor\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m    103\u001b[0m         input_format\n\u001b[1;32m    104\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39msize of input tensor and input format are different. \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m        tensor shape: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, input_format: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    106\u001b[0m         tensor\u001b[39m.\u001b[39mshape, input_format\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m     input_format \u001b[39m=\u001b[39m input_format\u001b[39m.\u001b[39mupper()\n\u001b[1;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(input_format) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: size of input tensor and input format are different.         tensor shape: (128, 784), input_format: CHW"
     ]
    }
   ],
   "source": [
    "cur_step = 0 # so luong buoc huan luyen\n",
    "mean_generator_loss = 0\n",
    "mean_discriminator_loss = 0\n",
    "test_generator = True # Whether the generator should be tested\n",
    "gen_loss = False\n",
    "error = False\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (real,_) in enumerate(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten\n",
    "        real = real.view(cur_batch_size, -1).to(device)\n",
    "        \n",
    "        ## Update discriminator\n",
    "        disc_optimazation.zero_grad()\n",
    "        disc_loss = get_dis_loss(generator, discriminator, criterion, real,\n",
    "                                  num_images=cur_batch_size, z_dim=z_dim, device=device)\n",
    "        # update gradien\n",
    "        disc_loss.backward() \n",
    "        # update optimizer\n",
    "        disc_optimazation.step()\n",
    "        if test_generator:\n",
    "            old_generator_weights = generator.gen[0][0].weight.detach().clone()\n",
    "        \n",
    "        # update generator\n",
    "        gen_optimazation.zero_grad()\n",
    "        gen_loss = get_gen_loss(generator, discriminator, criterion, cur_batch_size, z_dim, device)\n",
    "        gen_loss.backward()\n",
    "        gen_optimazation.step()\n",
    "\n",
    "        ## Kiem tra co thay doi weight cua generator hay khong\n",
    "        if test_generator:\n",
    "            try:\n",
    "                assert lr > 0.0000002 or (generator.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)\n",
    "                assert torch.any(generator.gen[0][0].weight.detach().clone() != old_generator_weights)\n",
    "            except:\n",
    "                error = True\n",
    "                print(\"Runtime tests have failed\")\n",
    "\n",
    "        mean_discriminator_loss += disc_loss.item() / display_step\n",
    "        mean_generator_loss += gen_loss.item() / display_step\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                      Loss D: {disc_loss:.4f}, loss G: {gen_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake_noise = get_noise(cur_batch_size, z_dim, device)\n",
    "                fake = generator(fake_noise)\n",
    "                data = real.reshape(-1,1,28,28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "                mean_generator_loss = 0\n",
    "                mean_discriminator_loss = 0\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", fake, global_step=cur_step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=cur_step\n",
    "                )\n",
    "                cur_step += 1\n",
    "\n",
    "    # # visualizing result\n",
    "    # if cur_step % display_step == 0 and cur_step > 0:\n",
    "    #     print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n",
    "    #     fake_noise = get_noise(cur_batch_size, z_dim, device)\n",
    "    #     fake = generator(fake_noise)\n",
    "        show_tensor_images(fake)\n",
    "    #     show_tensor_images(real)\n",
    "    #     mean_generator_loss = 0\n",
    "    #     mean_discriminator_loss = 0\n",
    "    \n",
    "    # cur_step += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
